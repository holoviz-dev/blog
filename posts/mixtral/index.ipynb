{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1669e852-3e4c-4819-a9ee-bfedd2ab62ed",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Build a Mixtral Chatbot with Panel\"\n",
    "date: \"2023-12-12\"\n",
    "description: \"With Transformers and llama.cpp\"\n",
    "author:\n",
    "  - Andrew Huang\n",
    "  - Philipp Rudiger\n",
    "  - Sophia Yang\n",
    "categories: [showcase, panel]\n",
    "image: \"images/chatbot.png\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb68fb53-5c93-449a-bf16-edced92ff738",
   "metadata": {},
   "source": [
    "Mistral AI just announced the Mixtral 8x7B and the Mixtral 8x7B Instruct models. These models have shown really amazing performance, outforming Llama 2 and GPT 3.5 in many benchmarks. They've quicked became the most popular 7B parameter open weights models in the AI world. In this blog post, we will walk you through how to build AI chatbots with the Mixtral 8x7B Instruct model using the Panel chat interface. We will cover two methods: \n",
    "\n",
    "- Method 1: Run Mixtral with transformers (A100 required)\n",
    "- Method 2: Run Mixtral with llama.cpp (can run on Macbook)\n",
    "\n",
    "# Method 1: Run Mixtral with transformers (A100 required)\n",
    "\n",
    "The first method is to use the latest Transformers from HuggingFace. We adapted the code from this blog post: [https://huggingface.co/blog/mixtral](https://huggingface.co/blog/mixtral). \n",
    "\n",
    "The following code: \n",
    "\n",
    "- Defines the model `mistralai/Mixtral-8x7B-Instruct-v0.1`\n",
    "- Uses the Tokenizer from this model to format the input user message\n",
    "- Uses the transformers pipeline to specify the text-generation pipeline, the model, and the 4-bit quantization.\n",
    "- Uses the `TextStreamer` to stream the text output \n",
    "\n",
    "```python\n",
    "# REF: Code adapted from https://huggingface.co/blog/mixtral\n",
    "!pip install -U \"transformers==4.36.0\"  --upgrade\n",
    "!pip install accelerate\n",
    "!pip install bitsandbytes\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "   \"text-generation\",\n",
    "   model=model,\n",
    "   model_kwargs={\"torch_dtype\": torch.float16, \"load_in_4bit\": True},\n",
    ")\n",
    "messages = [{\"role\": \"user\", \"content\": \"Explain what a Mixture of Experts is in less than 100 words.\"}]\n",
    "prompt = pipeline.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "from transformers import TextStreamer\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "outputs = pipeline(prompt, streamer=streamer, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "```\n",
    "\n",
    "We ran the code using one A100 GPU:\n",
    "\n",
    "<img src=\"./images/transformers.png\" width=\"60%\" style=\"margin-left: auto; margin-right: auto; display: block;\"></img>\n",
    "\n",
    "## Build a Panel chatbot \n",
    "- We wrap the code above in a function `callback`.\n",
    "- The key of building a Panel chatbot is to define `pn.chat.ChatInterface`. Specifically, in the `callback` method, we need to define how the chat bot responds to user message -- the `callback` function.\n",
    "- To turn a Python file or a notebook into a deployable app, simply append `.servable()` to the Panel object `chat_interface`.\n",
    "\n",
    "```python\n",
    "import panel as pn\n",
    "from transformers import AutoTokenizer, TextStreamer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "pn.extension()\n",
    "\n",
    "async def callback(contents: str, user: str, instance: pn.chat.ChatInterface):\n",
    "    messages = [{\"role\": \"user\", \"content\": content}]\n",
    "    prompt = pipeline.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "    outputs = pipeline(prompt, streamer=streamer, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "    message = \"\"\n",
    "    for token in outputs[0][\"generated_text\"]:\n",
    "        message += token\n",
    "        yield message\n",
    "        \n",
    "model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    model_kwargs={\"torch_dtype\": torch.float16, \"load_in_4bit\": True},\n",
    ")\n",
    "chat_interface = pn.chat.ChatInterface(callback=callback, callback_user=\"Mixtral\")\n",
    "chat_interface.send(\n",
    "    \"Send a message to get a reply from Mixtral!\", user=\"System\", respond=False\n",
    ")\n",
    "chat_interface.servable()\n",
    "```\n",
    "\n",
    "To launch a server using CLI and interact with this app, simply run `panel serve app.py`. Here is a example of the model's answer when we asked it to write a poem about Mistral AI. \n",
    "\n",
    "<img src=\"./images/transformers1.png\" width=\"60%\" style=\"margin-left: auto; margin-right: auto; display: block;\"></img>\n",
    "\n",
    "# Method 2: Run Mixtral with llama.cpp (run on Macbook)\n",
    "\n",
    "## Set up\n",
    "\n",
    "First, we need to download `llama-cpp-python`, which is a Python binding for llama.cpp. Depending on your computer, the steps to install it might look different. Since we are using a Macbook M1 Pro with a Metal GPU. Here are the steps to install `llama-cpp-python` with Metal: [https://llama-cpp-python.readthedocs.io/en/latest/install/macos/](https://llama-cpp-python.readthedocs.io/en/latest/install/macos/). Here is what I did:\n",
    "```console\n",
    "!CMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 pip install llama-cpp-python\n",
    "```\n",
    "\n",
    "Second, let's download the 4-bit quantized version of the Mixtral-8x7B-Instruct model form [Hugging Face](https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/blob/main/mixtral-8x7b-instruct-v0.1.Q4_0.gguf). Note that this file is quite big, about 26GB. \n",
    "```console\n",
    "wget https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q4_0.gguf\n",
    "```\n",
    "\n",
    "Because Mixtral is not merged in llama.cpp yet, we need to do the following steps. \n",
    "```console\n",
    "# REF: https://github.com/abetlen/llama-cpp-python/issues/1000\n",
    "git clone https://github.com/ggerganov/llama.cpp\n",
    "cd llama.cpp\n",
    "git checkout mixtral\n",
    "make -j\n",
    "make libllama.so\n",
    "```\n",
    "\n",
    "Finally, don't forget to install other needed packages such as `transformers` and `panel`.\n",
    "\n",
    "## Run Mixtral \n",
    "\n",
    "Below is the Python code for running Mixtral with llama.cpp. Here are the steps:\n",
    "\n",
    "- We first need to define an environment variable LLAMA_CPP_LIB directed to the libllama.so file, which is saved under the llama.cpp directly we got from `git clone` earlier.\n",
    "- Then we define our `llm` pointing to the `mixtral-8x7b-instruct-v0.1.Q4_0.gguf` file we downloaded from `wget`.\n",
    "- Note that we need load the `tokenizer` from the Mixtral-8x7B-Instruct model and format the input text the way the model expects. \n",
    "- Then we can get responses from `llm.create_completion`. The default `max_tokens` is 16. To get resonable good responses, let's increase this number to 256.\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.environ[\"LLAMA_CPP_LIB\"] = \"/PATH WHERE YOU SAVED THE llama.cpp DIRECTORY FROM GIT CLONE/llama.cpp/libllama.so\"\n",
    "\n",
    "from llama_cpp import Llama\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=\"./mixtral-8x7b-instruct-v0.1.Q4_0.gguf\",\n",
    "    n_gpu_layers=0,\n",
    ")\n",
    "\n",
    "model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "messages = [{\"role\": \"user\", \"content\": \"Explain what a Mixture of Experts is in less than 100 words.\"}]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "response = llm.create_completion(prompt, max_tokens=256)\n",
    "response['choices'][0]['text']\n",
    "```\n",
    "\n",
    "Here you can see the code running in Jupyter Notebook cells. Please be patient as this will take some time. After a few minutes, the model outputs results based on our input prompt:\n",
    "\n",
    "<img src=\"./images/llamacpp.png\" width=\"90%\" style=\"margin-left: auto; margin-right: auto; display: block;\"></img>\n",
    "\n",
    "\n",
    "## Build a Panel chatbot \n",
    "\n",
    "- Same as what we have seen in method 1, let's wrap the code logics above in a function called `callback`, which is how we want our chatbot to respond to user messages.\n",
    "- Then in `pn.chat.ChatInterface`, we define `callback` as this `callback` function.\n",
    "\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.environ[\"LLAMA_CPP_LIB\"] = \"/PATH WHERE YOU SAVED THE llama.cpp DIRECTORY FROM GIT CLONE/llama.cpp/libllama.so\"\n",
    "\n",
    "from llama_cpp import Llama\n",
    "from transformers import AutoTokenizer\n",
    "import panel as pn\n",
    "\n",
    "pn.extension()\n",
    "\n",
    "async def callback(contents: str, user: str, instance: pn.chat.ChatInterface):\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": contents}]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    response = llm.create_completion(prompt, max_tokens=256, stream=True)\n",
    "\n",
    "    message = \"\"\n",
    "    for chunk in response:\n",
    "        message += chunk['choices'][0]['text']\n",
    "        yield message\n",
    "        \n",
    "model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "llm = Llama(\n",
    "    model_path=\"./mixtral-8x7b-instruct-v0.1.Q4_0.gguf\",\n",
    "    n_gpu_layers=0,\n",
    ")\n",
    "chat_interface = pn.chat.ChatInterface(\n",
    "    callback=callback, \n",
    "    callback_user=\"Mixtral\",\n",
    "    message_params={\"show_reaction_icons\": False}\n",
    "    )\n",
    "chat_interface.send(\n",
    "    \"Send a message to get a reply from Mixtral!\", user=\"System\", respond=False\n",
    ")\n",
    "chat_interface.servable()\n",
    "```\n",
    "\n",
    "\n",
    "- Finally, we can run `panel serve app.py` to interact with this app. As you can see in this gif, it's actually quite slow generating each word because we are running on a local Macbook. \n",
    "<img src=\"./images/llamacpp.gif\" width=\"70%\" style=\"margin-left: auto; margin-right: auto; display: block;\"></img>\n",
    "\n",
    "\n",
    "\n",
    "# Conclusion\n",
    "In this blog post, we used transformers, llama.cpp, and Panel to create AI chatbots that uses the Mixtral 8x7B Instruct model. Whether you have an A100 GPU or a Macbook, you can try Mixtral 8x7B Instruct model and build chatbots right away.\n",
    "\n",
    "\n",
    "If you are interested in learning more about how to build AI chatbot in Panel, please read our related blog posts: \n",
    "\n",
    "- [Building AI Chatbots with Mistral and Llama2](https://medium.com/@sophiamyang/building-ai-chatbots-with-mistral-and-llama2-9c0f5abc296c) \n",
    "- [Building a Retrieval Augmented Generation Chatbot](https://medium.com/@sophiamyang/building-a-retrieval-augmented-generation-chatbot-d567a24fcd14)\n",
    "- [How to Build Your Own Panel AI Chatbots](https://medium.com/@sophiamyang/how-to-build-your-own-panel-ai-chatbots-ef764f7f114e)\n",
    "- [Build a RAG chatbot to answer questions about Python libraries](https://blog.holoviz.org/posts/fleet_ai/)\n",
    "\n",
    "If you find Panel useful, please consider giving us a star on Github ([https://github.com/holoviz/panel](https://github.com/holoviz/panel)). Happy coding! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
