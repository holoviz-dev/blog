{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1669e852-3e4c-4819-a9ee-bfedd2ab62ed",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Build a RAG chatbot to answer questions about Python libraries\"\n",
    "date: \"2023-12-07\"\n",
    "description: \"Access the Python universe with Fleet Context and Panel\"\n",
    "author:\n",
    "  - Andrew Huang\n",
    "  - Sophia Yang\n",
    "categories: [showcase, panel]\n",
    "image: \"images/chatbot.png\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb68fb53-5c93-449a-bf16-edced92ff738",
   "metadata": {},
   "source": [
    "Interested in asking questions about Python‚Äôs latest and greatest libraries? This is the chatbot for you! Fleet Context offers 4M+ high-quality custom embeddings of the top 1000+ Python libraries, while Panel can provide a Chat Interface UI to build a Retrieval-Augmented Generation (RAG) chatbot with Fleet Context.\n",
    "\n",
    "Why is this chatbot useful? It‚Äôs because most language models are not trained on the most up-to-date Python package docs and thus do not have information about the recent Python libraries like llamaindex, LangChain, etc. To be able to answer questions about these libraries, we can retrieve relevant information from Python library docs and generate valid and improved responses based on retrieved information.\n",
    "\n",
    "**Run the app**: [https://huggingface.co/spaces/ahuang11/panel-fleet](https://huggingface.co/spaces/ahuang11/panel-fleet) \\\n",
    "**Code**: [https://huggingface.co/spaces/ahuang11/panel-fleet/tree/main](https://huggingface.co/spaces/ahuang11/panel-fleet)\n",
    "\n",
    "<br />\n",
    "\n",
    "<div style=\"font-color: lightgray; font-size: 1em;\">\n",
    "<img src=\"./images/fleet1.gif\" width=\"100%\" style=\"margin-left: auto; margin-right: auto; display: block;\"></img>\n",
    "<center>Demo of the Python Library Document RAG Chatbot.</center>\n",
    "</div>\n",
    "\n",
    "\n",
    "# Access the entire Python universe with Fleet Context\n",
    "Just a few weeks ago, Fleet AI launched Fleet Context, a CLI and open-source corpus for the python ecosystem with 4M+ high quality embeddings of the top 1000+ Python Libraries. Currently, it offers embeddings for all top 1225 Python libraries and it‚Äôs adding more libraries and versions every day. All the embeddings can be downloaded here. Very impressive and useful! \n",
    "\n",
    "How do we use Fleet Context to ask questions about 1000+ Python packages? After installing the package `pip install fleet-context`, we can run Fleet Context either in the command line interface or in a Python console: \n",
    "\n",
    "## 1. Command line interface\n",
    " \n",
    "Once we define the OpenAI environment variable `export OPENAI_API_KEY=xxx`, we can run `context` in the command line and start ask questions about Python libraries. For example, here I asked ‚Äúwhat is HoloViz Panel?‚Äù. What I really like about Fleet is that it provides references for us to check.\n",
    "\n",
    "<img src=\"./images/fleet2.png\" width=\"100%\" style=\"margin-left: auto; margin-right: auto; display: block;\"></img>\n",
    "\n",
    "## 2. Python console\n",
    "\n",
    "We can query embeddings directly from the provided hosted vector database with the `query` method from the `context` library. When we ask a question ‚ÄúWhat is HoloViz Panel?‚Äù, it returned defined number (k=2) of related text chunks from the Panel docs. \n",
    "\n",
    "Note that the returned results include many metadata such as library_id, page_id, parent, section_id, title, text, type, etc., which are available for us to use and query. \n",
    "\n",
    "\n",
    "<img src=\"./images/fleet3.png\" width=\"100%\" style=\"margin-left: auto; margin-right: auto; display: block;\"></img>\n",
    "\n",
    "\n",
    "# Build a Panel chatbot to ask questions about Python libraries \n",
    "\n",
    "Let‚Äôs build a Panel chatbot UI of the Fleet Context with the following three steps: \n",
    "\n",
    "## 0. Import packages\n",
    "Before we get started, let's make sure we install the needed packages and import the packages:\n",
    "\n",
    "```python\n",
    "from context import query\n",
    "from openai import AsyncOpenAI\n",
    "import panel as pn\n",
    "```\n",
    "\n",
    "## 1. Define the system prompt\n",
    "\n",
    "Full credit to the Fleet Context team, we took this system prompt and tweaked it a bit from their code:\n",
    "\n",
    "\n",
    "```python\n",
    "# taken from fleet context\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert in Python libraries. You carefully provide accurate, factual, thoughtful, nuanced answers, and are brilliant at reasoning. If you think there might not be a correct answer, you say so.\n",
    "Each token you produce is another opportunity to use computation, therefore you always spend a few sentences explaining background context, assumptions, and step-by-step thinking BEFORE you try to answer a question.\n",
    "Your users are experts in AI and ethics, so they already know you're a language model and your capabilities and limitations, so don't remind them of that. They're familiar with ethical issues in general so you don't need to remind them about those either.\n",
    "Your users are also in a CLI environment. You are capable of writing and running code. DO NOT write hypothetical code. ALWAYS write real code that will execute and run end-to-end.\n",
    "Instructions:\n",
    "- Be objective, direct. Include literal information from the context, don't add any conclusion or subjective information.\n",
    "- When writing code, ALWAYS have some sort of output (like a print statement). If you're writing a function, call it at the end. Do not generate the output, because the user can run it themselves.\n",
    "- ALWAYS cite your sources. Context will be given to you after the text ### Context source_url ### with source_url being the url to the file. For example, ### Context https://example.com/docs/api.html#files ### will have a source_url of https://example.com/docs/api.html#files.\n",
    "- When you cite your source, please cite it as [num] with `num` starting at 1 and incrementing with each source cited (1, 2, 3, ...). At the bottom, have a newline-separated `num: source_url` at the end of the response. ALWAYS add a new line between sources or else the user won't be able to read it. DO NOT convert links into markdown, EVER! If you do that, the user will not be able to click on the links.\n",
    "For example:\n",
    "**Context 1**: https://example.com/docs/api.html#pdfs\n",
    "I'm a big fan of PDFs.\n",
    "**Context 2**: https://example.com/docs/api.html#csvs\n",
    "I'm a big fan of CSVs.\n",
    "### Prompt ###\n",
    "What is this person a big fan of?\n",
    "### Response ###\n",
    "This person is a big fan of PDFs[1] and CSVs[2].\n",
    "1: https://example.com/docs/api.html#pdfs\n",
    "2: https://example.com/docs/api.html#csvs\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "## 2. Define chat interface\n",
    "\n",
    "The key component of defining a Panel chat interface is `pn.chat.ChatInterface`. Specifically, in the `callback` method, we need to define how the chat bot responds ‚Äì the `answer` function.\n",
    "\n",
    "In this function, we:\n",
    "- Initialize the system prompt\n",
    "- Used the Fleet Context `query` method to query k=3 relevant text chunks for our given question\n",
    "- We format the retrieved text chunks, URLs, and user message into the required OpenAI message format\n",
    "- We provide the message history into an OpenAI model.\n",
    "- Then we stream the responses asynchronously from OpenAI. \n",
    "\n",
    "```python\n",
    "async def answer(contents, user, instance):\n",
    "    # start with system prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "\n",
    "    # add context to the user input\n",
    "    context = \"\"\n",
    "    fleet_responses = query(contents, k=3)\n",
    "    for i, response in enumerate(fleet_responses):\n",
    "        context += f\"\\n\\n**Context {i}**: {response['metadata']['url']}\\n{response['metadata']['text']}\"\n",
    "    instance.send(context, avatar=\"üõ©Ô∏è\", user=\"Fleet Context\", respond=False)\n",
    "\n",
    "    # get history of messages (skipping the intro message)\n",
    "    # and serialize fleet context messages as \"user\" role\n",
    "    messages.extend(\n",
    "        instance.serialize(role_names={\"user\": [\"user\", \"Fleet Context\"]})[1:]\n",
    "    )\n",
    "\n",
    "    openai_response = await client.chat.completions.create(\n",
    "        model=MODEL, messages=messages, temperature=0.2, stream=True\n",
    "    )\n",
    "\n",
    "    message = \"\"\n",
    "    async for chunk in openai_response:\n",
    "        token = chunk.choices[0].delta.content\n",
    "        if token:\n",
    "            message += token\n",
    "            yield message\n",
    "\n",
    "\n",
    "client = AsyncOpenAI()\n",
    "intro_message = pn.chat.ChatMessage(\"Ask me anything about Python libraries!\", user=\"System\")\n",
    "chat_interface = pn.chat.ChatInterface(intro_message, callback=answer, callback_user=\"OpenAI\")\n",
    "```\n",
    "\n",
    "## 3. Format everything in a template \n",
    "Finally we format everything in a template to get the final app: \n",
    "```python\n",
    "template = pn.template.FastListTemplate(main=[chat_interface], title=\"Panel UI of Fleet Context üõ©Ô∏è\")\n",
    "template.servable()\n",
    "```\n",
    "\n",
    "<div style=\"font-color: lightgray; font-size: 1em;\">\n",
    "<img src=\"./images/fleet1.gif\" width=\"100%\" style=\"margin-left: auto; margin-right: auto; display: block;\"></img>\n",
    "<center>Demo of the Python Library Document RAG Chatbot.</center>\n",
    "</div>\n",
    "\n",
    "<br />\n",
    "\n",
    "Now, you should have a working AI chatbot that can answer questions about Python libraries. If you would like to add more complex RAG features. LlamaIndex has incorporated it into its system. Here is a guide if you would like to experiment Fleet Context with LlamaIndex: [Fleet Context Embeddings - Building a Hybrid Search Engine for the Llamaindex Library](https://docs.llamaindex.ai/en/stable/community/integrations/fleet_libraries_context.html). \n",
    "\n",
    "# Conclusion\n",
    "In this blog post, we used Fleet Context and Panel to create a RAG chatbot that has access to the Python universe. We can ask any questions about the top 1000+ Python packages, get answers, and responses from the package docs. \n",
    "\n",
    "If you are interested in learning more about how to build AI chatbot in Panel, please read our related blog posts: \n",
    "\n",
    "- [Building AI Chatbots with Mistral and Llama2](https://medium.com/@sophiamyang/building-ai-chatbots-with-mistral-and-llama2-9c0f5abc296c) \n",
    "- [Building a Retrieval Augmented Generation Chatbot](https://medium.com/@sophiamyang/building-a-retrieval-augmented-generation-chatbot-d567a24fcd14)\n",
    "- [How to Build Your Own Panel AI Chatbots](https://medium.com/@sophiamyang/how-to-build-your-own-panel-ai-chatbots-ef764f7f114e) \n",
    "\n",
    "If you find Panel useful, please consider giving us a star on Github ([https://github.com/holoviz/panel](https://github.com/holoviz/panel)). Happy coding! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df75d514-d7b0-4c1f-8fe4-604b252057bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
